{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CheckMaite\n",
    "## Intro\n",
    "\n",
    "After 2021, with the global deployment of LLM solution ChatGPT, a constatation has been made : about 80% of newly published abstracts were likely to be generated by AI. \n",
    "\n",
    "The goal of this report is not to judge whether this is good or bad news for research, knowing that although quality can be deteriored, and abstracts normalized, chatgpt opened the domain to new countries where english is not well spoken.\n",
    "\n",
    "The goal of this project is to train a model that could be used to detect whether an abstract is written by a real person, or by an AI tool.\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "The data used in this project is AI-GA Dataset (https://paperswithcode.com/dataset/ai-ga-ai-generated-abstracts-dataset), for which no benchmark has been created yet.\n",
    "\n",
    "It is structured under csv format : title, text, label and contains 28663 rows.\n",
    "\n",
    "Because the model we will use in the next parts needs to be under the shape of vectors, we need to preprocess each row. We use an open-source **text embedding** for that (https://huggingface.co/thenlper/gte-base). Because this embedding model only supports 512 tokens-length text, the question of how we will create the vectors is a first point of discussion. We decide in first time to keep only the first 512 tokens to build the embeddings. In a second time, we will create embeddings for all tokens, and train the model on that full set. When testing that method, we will use the gliding window technique and consider that the text is generated by AI if at least 1 of its constitute part is detected generated by AI.\n",
    "\n",
    "The considered potential input for the model are : title embedding, abstract text embedding. In addition, if time allows us to do so, we will consider adding the lab associated to the publication, the research journal or conference, and the date of publish. It will require additional processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brainlab/Documents/gitlab/CheckMaite/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OBJECTIVE: This retrospective chart review describes the epidemiology and clinical features of 40 patients with culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia. METHODS: Patients with positive M. pneumoniae cultures from respiratory specimens from January 1997 through December 1998 were identified through the Microbiology records. Charts of patients were reviewed. RESULTS: 40 patients were identified, 33 (82.5%) of whom required admission. Most infections (92.5%) were community-acquired. The infection affected all age groups but was most common in infants (32.5%) and pre-school children (22.5%). It occurred year-round but was most common in the fall (35%) and spring (30%). More than three-quarters of patients (77.5%) had comorbidities. Twenty-four isolates (60%) were associated with pneumonia, 14 (35%) with upper respiratory tract infections, and 2 (5%) with bronchiolitis. Cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs. Most patients with pneumonia had crepitations (79.2%) but only 25% had bronchial breathing. Immunocompromised patients were more likely than non-immunocompromised patients to present with pneumonia (8/9 versus 16/31, P = 0.05). Of the 24 patients with pneumonia, 14 (58.3%) had uneventful recovery, 4 (16.7%) recovered following some complications, 3 (12.5%) died because of M pneumoniae infection, and 3 (12.5%) died due to underlying comorbidities. The 3 patients who died of M pneumoniae pneumonia had other comorbidities. CONCLUSION: our results were similar to published data except for the finding that infections were more common in infants and preschool children and that the mortality rate of pneumonia in patients with comorbidities was high.', 'Inflammatory diseases of the respiratory tract are commonly associated with elevated production of nitric oxide (NO•) and increased indices of NO• -dependent oxidative stress. Although NO• is known to have anti-microbial, anti-inflammatory and anti-oxidant properties, various lines of evidence support the contribution of NO• to lung injury in several disease models. On the basis of biochemical evidence, it is often presumed that such NO• -dependent oxidations are due to the formation of the oxidant peroxynitrite, although alternative mechanisms involving the phagocyte-derived heme proteins myeloperoxidase and eosinophil peroxidase might be operative during conditions of inflammation. Because of the overwhelming literature on NO• generation and activities in the respiratory tract, it would be beyond the scope of this commentary to review this area comprehensively. Instead, it focuses on recent evidence and concepts of the presumed contribution of NO• to inflammatory diseases of the lung.', 'Surfactant protein-D (SP-D) participates in the innate response to inhaled microorganisms and organic antigens, and contributes to immune and inflammatory regulation within the lung. SP-D is synthesized and secreted by alveolar and bronchiolar epithelial cells, but is also expressed by epithelial cells lining various exocrine ducts and the mucosa of the gastrointestinal and genitourinary tracts. SP-D, a collagenous calcium-dependent lectin (or collectin), binds to surface glycoconjugates expressed by a wide variety of microorganisms, and to oligosaccharides associated with the surface of various complex organic antigens. SP-D also specifically interacts with glycoconjugates and other molecules expressed on the surface of macrophages, neutrophils, and lymphocytes. In addition, SP-D binds to specific surfactant-associated lipids and can influence the organization of lipid mixtures containing phosphatidylinositol in vitro. Consistent with these diverse in vitro activities is the observation that SP-D-deficient transgenic mice show abnormal accumulations of surfactant lipids, and respond abnormally to challenge with respiratory viruses and bacterial lipopolysaccharides. The phenotype of macrophages isolated from the lungs of SP-D-deficient mice is altered, and there is circumstantial evidence that abnormal oxidant metabolism and/or increased metalloproteinase expression contributes to the development of emphysema. The expression of SP-D is increased in response to many forms of lung injury, and deficient accumulation of appropriately oligomerized SP-D might contribute to the pathogenesis of a variety of human lung diseases.', 'Endothelin-1 (ET-1) is a 21 amino acid peptide with diverse biological activity that has been implicated in numerous diseases. ET-1 is a potent mitogen regulator of smooth muscle tone, and inflammatory mediator that may play a key role in diseases of the airways, pulmonary circulation, and inflammatory lung diseases, both acute and chronic. This review will focus on the biology of ET-1 and its role in lung disease.', 'Respiratory syncytial virus (RSV) and pneumonia virus of mice (PVM) are viruses of the family Paramyxoviridae, subfamily pneumovirus, which cause clinically important respiratory infections in humans and rodents, respectively. The respiratory epithelial target cells respond to viral infection with specific alterations in gene expression, including production of chemoattractant cytokines, adhesion molecules, elements that are related to the apoptosis response, and others that remain incompletely understood. Here we review our current understanding of these mucosal responses and discuss several genomic approaches, including differential display reverse transcription-polymerase chain reaction (PCR) and gene array strategies, that will permit us to unravel the nature of these responses in a more complete and systematic manner.']\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "dataset_url = \"dataset/ai-ga-dataset.csv\"\n",
    "input_texts = pd.read_csv(dataset_url, usecols=['abstract']).values.flatten().tolist()\n",
    "\n",
    "print(input_texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-base\")\n",
    "model = AutoModel.from_pretrained(\"thenlper/gte-base\")\n",
    "\n",
    "# loop and save embeddings in a file named embeddings.pt\n",
    "for i in range(0, len(input_texts)):\n",
    "    batch_dict = tokenizer(input_texts[i], max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    outputs = model(**batch_dict)\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    with open(f'embeddings/embeddings{i}.pt', 'ab') as f:\n",
    "        torch.save(embeddings, f)\n",
    "    with open('labels.txt', 'a') as f:\n",
    "        f.write(f'{i}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_titles = pd.read_csv(dataset_url, usecols=['title']).values.flatten().tolist()\n",
    "\n",
    "for i in range(0, len(input_titles)):\n",
    "    batch_dict = tokenizer(input_titles[i], max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    outputs = model(**batch_dict)\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    torch.save(embeddings, f'embeddings/embeddings_titles{i}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_titles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m     embeddings.append(torch.load(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33membeddings/embeddings\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pt\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      6\u001b[39m embeddings_titles = []\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43minput_titles\u001b[49m)):\n\u001b[32m      8\u001b[39m     embeddings_titles.append(torch.load(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33membeddings/embeddings_titles\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pt\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# labels\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'input_titles' is not defined"
     ]
    }
   ],
   "source": [
    "# load embeddings from the file\n",
    "input_titles = pd.read_csv(dataset_url, usecols=['title']).values.flatten().tolist()\n",
    "embeddings = []\n",
    "for i in range(0, len(input_texts)):\n",
    "    embeddings.append(torch.load(f'embeddings/embeddings{i}.pt'))\n",
    "\n",
    "embeddings_titles = []\n",
    "for i in range(0, len(input_titles)):\n",
    "    embeddings_titles.append(torch.load(f'embeddings/embeddings_titles{i}.pt'))\n",
    "\n",
    "# labels\n",
    "\n",
    "labels = pd.read_csv(dataset_url, usecols=['label']).values.flatten().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embeddings))\n",
    "print(len(embeddings_titles))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "input_embeddings = torch.cat(embeddings).detach().numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_embeddings, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print(f'Random Forest accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=['H', 'AI'], yticklabels=['H', 'AI'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "print(f'SVM accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=['H', 'AI'], yticklabels=['H', 'AI'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "print(f'XGBoost accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "#display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=['H', 'AI'], yticklabels=['H', 'AI'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train xgboost, random forest tree and svm models using both input embeddings and title embeddings\n",
    "\n",
    "inputs = []\n",
    "for i in range(0, len(embeddings)):\n",
    "    inputs.append(torch.cat((embeddings[i], embeddings_titles[i]), dim=1).squeeze().detach().numpy())\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print(f'Random Forest accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "# SVM\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "print(f'SVM accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "# XGBoost\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "print(f'XGBoost accuracy: {accuracy_score(y_test, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
